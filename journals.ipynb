{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Journals\n",
    "\n",
    "This notebook is used for processing daily journals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "# These will be specific to the journal data file used\n",
    "JOURNAL_FILENAME = 'journals.txt'\n",
    "HEADER_REGEX = r'(\\d+)\\/(\\d+)\\/(\\d+)'\n",
    "GRATEFUL_REGEX = r'\\-(.*)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets put the journals into two seperate dataframes. One will include the bulleted grateful items for that day. The other will include the raw paragraphs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_paragraphs = []\n",
    "raw_gratefuls = []\n",
    "\n",
    "date = 'Empty'\n",
    "grateful = 'Empty'\n",
    "paragraph = 'Empty'\n",
    "\n",
    "# Section headers (not to be included)\n",
    "headers = ['\\n', '#journal\\n', '#daily\\n']\n",
    "\n",
    "# Import journal data from text file\n",
    "with open(JOURNAL_FILENAME) as file:\n",
    "    for line in file:\n",
    "        if line.endswith(':\\n'):\n",
    "            continue\n",
    "        if 'Journal' in line:\n",
    "            raw_date = re.search(HEADER_REGEX, line).groups()\n",
    "            date = '/'.join(raw_date)\n",
    "            weekday = datetime.datetime(int(raw_date[2]),\n",
    "                                        int(raw_date[1]),\n",
    "                                        int(raw_date[0])).weekday()\n",
    "        elif line.startswith('-'):\n",
    "            grateful = re.search(GRATEFUL_REGEX, line).group(1)\n",
    "            raw_gratefuls.append((date, weekday, grateful))\n",
    "        elif line not in headers:\n",
    "            raw_paragraphs.append((date, weekday, line))\n",
    "        else:\n",
    "            pass\n",
    "                \n",
    "# Convert to dataframes\n",
    "grateful = pd.DataFrame(raw_gratefuls, columns=['date', 'weekday', 'text'])\n",
    "paragraph = pd.DataFrame(raw_paragraphs, columns=['date', 'weekday', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out dataframes to see data\n",
    "print(paragraph)\n",
    "print(grateful)\n",
    "grateful.head()\n",
    "paragraph.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Lets convert each of the paragraphs into a list of tokens.\n",
    "\n",
    "Sources:\n",
    "- [1] http://pythondata.com/text-analytics-visualization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from string import punctuation\n",
    "\n",
    "# Download nltk resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Use nltk to set up stopwords, stemmer, and lemmatizer\n",
    "stop = nltk.corpus.stopwords.words('english')\n",
    "porter = nltk.stem.PorterStemmer()\n",
    "wnl = nltk.stem.WordNetLemmatizer() \n",
    "\n",
    "def tokenizer(text):\n",
    " \n",
    "    tokens_ = [nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text)]\n",
    " \n",
    "    tokens = []\n",
    "    for token_by_sent in tokens_:\n",
    "        tokens += token_by_sent\n",
    " \n",
    "    tokens = list(filter(lambda t: t.lower() not in stop, tokens))\n",
    "    tokens = list(filter(lambda t: t not in punctuation, tokens))\n",
    "    tokens = list(filter(lambda t: t not in [u\"'s\", u\"n't\", u\"...\", u\"''\", u'``', u'\\u2014', u'\\u2026', u'\\u2013'], tokens))\n",
    "     \n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        token = wnl.lemmatize(token)\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    " \n",
    "    filtered_tokens = list(map(lambda token: token.lower(), filtered_tokens))\n",
    " \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the map method to create new columns with tokens\n",
    "grateful['tokens'] = grateful['text'].map(tokenizer)\n",
    "paragraph['tokens'] = paragraph['text'].map(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords\n",
    "\n",
    "Lets get the most common tokens for each text, and use those as keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def keywords(tokens, num=2):\n",
    "    return Counter(tokens).most_common(num)\n",
    "\n",
    "# Use the map method to create new columns with keywords\n",
    "grateful['keywords'] = grateful['tokens'].map(keywords)\n",
    "paragraph['keywords'] = paragraph['tokens'].map(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keep]",
   "language": "python",
   "name": "conda-env-keep-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
