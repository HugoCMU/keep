{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Journals\n",
    "\n",
    "This notebook is used for processing daily journals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "# These will be specific to the journal data file used\n",
    "JOURNAL_FILENAME = 'journals.txt'\n",
    "HEADER_REGEX = r'(\\d+)\\/(\\d+)\\/(\\d+)'\n",
    "GRATEFUL_REGEX = r'\\-(.*)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets put the journals into two seperate dataframes. One will include the bulleted grateful items for that day. The other will include the raw paragraphs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_paragraphs = []\n",
    "raw_gratefuls = []\n",
    "\n",
    "date = 'Empty'\n",
    "grateful = 'Empty'\n",
    "paragraph = 'Empty'\n",
    "\n",
    "# Section headers (not to be included)\n",
    "headers = ['\\n', '#journal\\n', '#daily\\n']\n",
    "\n",
    "# Import journal data from text file\n",
    "with open(JOURNAL_FILENAME) as file:\n",
    "    for line in file:\n",
    "        # Remove unicode escape characters when necessary\n",
    "#         line = line.strip(u'\\u')\n",
    "        if line.endswith(':\\n'):\n",
    "            continue\n",
    "        if 'Journal' in line:\n",
    "            raw_date = re.search(HEADER_REGEX, line).groups()\n",
    "            date = '/'.join(raw_date)\n",
    "            weekday = datetime.datetime(int(raw_date[2]),\n",
    "                                        int(raw_date[1]),\n",
    "                                        int(raw_date[0])).weekday()\n",
    "        elif line.startswith('-'):\n",
    "            text = re.search(GRATEFUL_REGEX, line).group(1)\n",
    "            word_list = text.split()\n",
    "            raw_gratefuls.append((date, weekday, text, word_list))\n",
    "        elif line not in headers:\n",
    "            word_list = line.split()\n",
    "            raw_paragraphs.append((date, weekday, line, word_list))\n",
    "        else:\n",
    "            pass\n",
    "                \n",
    "# Convert to dataframes\n",
    "grateful = pd.DataFrame(raw_gratefuls, columns=['date', 'weekday', 'text', 'words'])\n",
    "paragraph = pd.DataFrame(raw_paragraphs, columns=['date', 'weekday', 'text', 'words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out dataframes to see data\n",
    "print(paragraph)\n",
    "print(grateful)\n",
    "grateful.head()\n",
    "paragraph.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Lets convert each of the paragraphs into a list of tokens.\n",
    "\n",
    "Sources:\n",
    "- [1] http://pythondata.com/text-analytics-visualization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from string import punctuation\n",
    "\n",
    "# Download nltk resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Use nltk to set up stopwords, stemmer, and lemmatizer\n",
    "stop = nltk.corpus.stopwords.words('english')\n",
    "porter = nltk.stem.PorterStemmer()\n",
    "wnl = nltk.stem.WordNetLemmatizer() \n",
    "\n",
    "def tokenizer(text):\n",
    " \n",
    "    tokens_ = [nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text)]\n",
    " \n",
    "    tokens = []\n",
    "    for token_by_sent in tokens_:\n",
    "        tokens += token_by_sent\n",
    " \n",
    "    tokens = list(filter(lambda t: t.lower() not in stop, tokens))\n",
    "    tokens = list(filter(lambda t: t not in punctuation, tokens))\n",
    "    tokens = list(filter(lambda t: t not in [u\"'s\", u\"n't\", u\"...\", u\"''\", u'``', u'\\u2014', u'\\u2026', u'\\u2013'], tokens))\n",
    "     \n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        token = wnl.lemmatize(token)\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    " \n",
    "    filtered_tokens = list(map(lambda token: token.lower(), filtered_tokens))\n",
    " \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the map method to create new columns with tokens\n",
    "grateful['tokens'] = grateful['text'].map(tokenizer)\n",
    "paragraph['tokens'] = paragraph['text'].map(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords\n",
    "\n",
    "Lets get the most common tokens for each text, and use those as keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def keywords(tokens, num=2):\n",
    "    return Counter(tokens).most_common(num)\n",
    "\n",
    "# Use the map method to create new columns with keywords\n",
    "grateful['keywords'] = grateful['tokens'].map(keywords)\n",
    "paragraph['keywords'] = paragraph['tokens'].map(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Stats\n",
    "\n",
    "Lets get some more statistics for each text, such as length of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Use the map method to create new columns with keywords\n",
    "grateful['num_words'] = grateful['text'].map(num_words)\n",
    "paragraph['num_words'] = paragraph['text'].map(num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot\n",
    "\n",
    "Make some plots of the statistics we have extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Histogram for the number of entries per day of the week\n",
    "grateful['weekday'].plot.hist(alpha=0.5)\n",
    "paragraph['weekday'].plot.hist(alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "We use the tensorflow data API to read in the text data line by line. Separating it into test, validate, and train as well as shuffling and batching it when appropriate.\n",
    "\n",
    "Sources:\n",
    "\n",
    "- [2] Tensorflow for Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "# List of all words in gratefuls\n",
    "grateful_list = [word for words in grateful['words'].tolist() for word in words]\n",
    "\n",
    "build_dataset(grateful_list, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T08:19:42.437910Z",
     "start_time": "2018-04-25T08:19:42.433036Z"
    }
   },
   "source": [
    "## Word Embedding\n",
    "\n",
    "Make a word embedding from our text data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "vocab_size = 1000\n",
    "embed_size = 32\n",
    "\n",
    "with tf.name_scope('inputs'):\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size,])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size,])\n",
    "\n",
    "with tf.name_scope('embeddings'):\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocab_size, embed_size], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "    \n",
    "with tf.name_scope('weights'):\n",
    "    nce_weights = tf.Variable(tf.truncated_normal([vocab_size, embed_size], stddev = 1.0 / math.sqrt(embed_size)))\n",
    "    \n",
    "with tf.name_scope('biases'):\n",
    "    nce_bias = tf.Variable(tf.zeros([vocab_size]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keep]",
   "language": "python",
   "name": "conda-env-keep-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
